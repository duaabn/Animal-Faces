{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176357,"sourceType":"datasetVersion","datasetId":667852}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1: Setup and Configuration","metadata":{}},{"cell_type":"markdown","source":"****This cell imports necessary libraries and defines the optimized hyperparameters, including the new resolution and unfreeze depth.****","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# --- 0. Configuration and Setup ---\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# --- Data Paths (Must be accurate for Kaggle environment) ---\nKAGGLE_ROOT_PATH = '/kaggle/input/animal-faces/afhq'\nTRAIN_DIR_FULL = KAGGLE_ROOT_PATH + '/train'\nTEST_DIR = KAGGLE_ROOT_PATH + '/val'\nCLASS_NAMES = [\"cat\", \"dog\", \"wild\"]\nNUM_CLASSES = len(CLASS_NAMES)\n\n# --- Optimized Hyperparameters ---\nTARGET_SIZE = (224, 224)    # Increased Resolution (Fix for Lion/Giraffe mix-up)\nBATCH_SIZE = 32\nVALIDATION_SPLIT_RATIO = 0.105\nINPUT_SHAPE = (TARGET_SIZE[0], TARGET_SIZE[1], 3)\nMODEL_SAVE_PATH = 'best_efficientnetb0_224_model.keras'\n\n# --- Training and Fine-Tuning Parameters ---\nINITIAL_EPOCHS = 10\nFINE_TUNE_EPOCHS = 20\nINITIAL_LR = 0.001\nFINE_TUNE_LR = 1e-5\nDROPOUT_RATE = 0.5\nDROPOUT_RATE_NEW = 0.4\nUNFREEZE_LAYERS = 100 # Deep Fine-Tuning Depth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2: Data Preparation and Generators","metadata":{}},{"cell_type":"markdown","source":"****sets up the data pipelines, ensuring the images are resized to the optimized $224 \\times 224$ resolution and appropriate augmentation is applied.****","metadata":{}},{"cell_type":"code","source":"# --- 1. Data Generators Setup ---\ntrain_val_datagen = ImageDataGenerator(\n    rescale=1./255, rotation_range=20, horizontal_flip=True, fill_mode='nearest',\n    validation_split=VALIDATION_SPLIT_RATIO\n)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Use the new TARGET_SIZE for all generators\ntrain_generator = train_val_datagen.flow_from_directory(\n    TRAIN_DIR_FULL, target_size=TARGET_SIZE, batch_size=BATCH_SIZE,\n    class_mode='categorical', classes=CLASS_NAMES, subset='training'\n)\nvalidation_generator = train_val_datagen.flow_from_directory(\n    TRAIN_DIR_FULL, target_size=TARGET_SIZE, batch_size=BATCH_SIZE,\n    class_mode='categorical', classes=CLASS_NAMES, subset='validation'\n)\ntest_generator = test_datagen.flow_from_directory(\n    TEST_DIR, target_size=TARGET_SIZE, batch_size=BATCH_SIZE,\n    class_mode='categorical', classes=CLASS_NAMES, shuffle=False\n)\n\nsteps_per_epoch_train = train_generator.samples // BATCH_SIZE\nvalidation_steps_val = validation_generator.samples // BATCH_SIZE","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3: Model Architecture and the BN Unfreeze Fix","metadata":{}},{"cell_type":"markdown","source":"********defines the EfficientNetB0 architecture and applies the critical Batch Normalization (BN) Unfreeze technique to ensure the model learns correctly in the first phase.********","metadata":{}},{"cell_type":"code","source":"# --- 2. Build EfficientNetB0 Model (Phase 1: BN Unfrozen) ---\nbase_model = EfficientNetB0(\n    weights='imagenet',\n    include_top=False,\n    input_shape=INPUT_SHAPE\n)\n\n# *** CRITICAL FIX: Partial Freezing (Unfreeze BN Layers only) ***\n# This allows the BN layers to adapt to the new image statistics (AFHQ) \n# while keeping the core convolutional weights frozen.\nbase_model.trainable = True \nfor layer in base_model.layers:\n    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = False\n    # BN layers remain trainable=True (default when trainable=True is set)\n\n# --- Build Custom Head Layers ---\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n\n# Two Dense layers with Dropout for robust regularization\nx = Dense(512, activation='relu')(x) \nx = Dropout(DROPOUT_RATE)(x) \nx = Dense(256, activation='relu')(x) \nx = Dropout(DROPOUT_RATE_NEW)(x) \n\npredictions = Dense(NUM_CLASSES, activation='softmax')(x) \nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# --- 3. Compile Model for Initial Training ---\nmodel.compile(optimizer=Adam(learning_rate=INITIAL_LR), loss='categorical_crossentropy', metrics=['accuracy'])\nprint(f\"\\nModel Ready. Input Resolution: {TARGET_SIZE}. Batch Norm layers are Unfrozen.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4: Initial Training and Deep Fine-Tuning Execution","metadata":{}},{"cell_type":"markdown","source":"********Executes the two training phases, including the deep fine-tuning for 100 layers to specialize the model on fine-grained animal features (Lion vs. Dog).********","metadata":{}},{"cell_type":"code","source":"# --- 4. Initial Training (Transfer Learning) ---\nprint(\"\\n--- Starting Initial Training (Phase 1) ---\")\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint(filepath=MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n\nhistory_initial = model.fit(\n    train_generator, steps_per_epoch=steps_per_epoch_train, epochs=INITIAL_EPOCHS,\n    validation_data=validation_generator, validation_steps=validation_steps_val,\n    callbacks=[early_stopping, model_checkpoint]\n)\n\n# --- 5. Fine-Tuning Setup (Phase 2: Deep Fine-Tuning) ---\nprint(\"\\n--- Phase 2: Fine-Tuning Setup ---\")\n\n# 1. Recreate the Base Model structure to load weights accurately\nbase_model_ft = EfficientNetB0(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n# Recreate Head Layers (must match Phase 1 structure)\nx = base_model_ft.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(DROPOUT_RATE)(x) \nx = Dense(256, activation='relu')(x)\nx = Dropout(DROPOUT_RATE_NEW)(x)\npredictions = Dense(NUM_CLASSES, activation='softmax')(x)\nmodel = Model(inputs=base_model_ft.input, outputs=predictions)\n\n# 2. Load best weights from Phase 1 checkpoint\ntry:\n    model.load_weights(MODEL_SAVE_PATH) \n    print(f\"Successfully loaded best weights from: {MODEL_SAVE_PATH}\")\nexcept Exception as e:\n    print(f\"Error loading weights: {e}\")\n\n# 3. Unfreeze 100 layers for specialized learning\nbase_model_ft.trainable = True\nnum_base_layers = len(base_model_ft.layers)\nfreeze_until = num_base_layers - UNFREEZE_LAYERS\n\n# Freeze the first part, unfreeze the final N layers\nfor layer in base_model_ft.layers[:freeze_until]:\n    layer.trainable = False\nfor layer in base_model_ft.layers[freeze_until:]:\n    layer.trainable = True\n\nprint(f\"Starting Fine-Tuning with {UNFREEZE_LAYERS} layers unfrozen.\")\n\n# 4. Recompile with low learning rate\nmodel.compile(optimizer=Adam(learning_rate=FINE_TUNE_LR), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# --- 7. Fine-Tuning Training --- \nprint(\"\\n--- Starting Fine-Tuning Training (Phase 2) ---\")\nhistory_fine = model.fit(\n    train_generator, steps_per_epoch=steps_per_epoch_train, epochs=FINE_TUNE_EPOCHS,\n    validation_data=validation_generator, validation_steps=validation_steps_val,\n    callbacks=[early_stopping, model_checkpoint]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5: Final Evaluation and Deployment Preparation","metadata":{}},{"cell_type":"markdown","source":"******performs the final evaluation on the test set, prints the classification report, and saves the final model for API deployment******","metadata":{}},{"cell_type":"code","source":"# --- 8. Final Evaluation, Report, and Visualization ---\n\n# 1. Final Evaluation on Test Set\nprint(\"\\n--- Final Model Evaluation on Test Set ---\")\ntest_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // BATCH_SIZE)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# 2. Detailed Classification Report (Crucial for verifying Lion/Dog fix)\nprint(\"\\n--- Detailed Classification Report ---\")\ntest_generator.reset()\nY_pred = model.predict(test_generator, steps=test_generator.samples // BATCH_SIZE + 1)\ny_pred_classes = np.argmax(Y_pred, axis=1)\ny_true_classes = test_generator.classes[:len(y_pred_classes)]\n\nreport = classification_report(y_true_classes, y_pred_classes, target_names=CLASS_NAMES, zero_division=0)\nprint(report)\n\n# 3. Final Model Save for Deployment\nSAVED_MODEL_DIR = './final_efficientnetb0_224_saved_model'\ntf.saved_model.save(model, SAVED_MODEL_DIR)\nprint(f\"\\nâœ¨ Final Model Saved for API Deployment: {SAVED_MODEL_DIR}\")\n\n# Optional: Plotting code goes here after the training is complete","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}